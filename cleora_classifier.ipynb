{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load Data\n",
    "# Replace with the actual file paths if they are in a specific folder.\n",
    "source_target_embeddings = pd.read_csv(\"output/emb__Source__Target.out\", delim_whitespace=True, header=None, skiprows=1)\n",
    "source_weight_embeddings = pd.read_csv(\"output/emb__Source__Weight.out\", delim_whitespace=True, header=None, skiprows=1)\n",
    "weight_target_embeddings = pd.read_csv(\"output/emb__Target__Weight.out\", delim_whitespace=True, header=None, skiprows=1)\n",
    "interaction_data = pd.read_csv(\"data/witcher_processed.tsv\", sep=\"\\t\")\n",
    "book_labels = pd.read_csv(\"data/witcher_target.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Step 1: Merge Data\n",
    "# Extract column names for embeddings (assuming embeddings start from column 2)\n",
    "source_target_embeddings.columns = [\"Node\"] + [f\"Embed_{i}\" for i in range(1, source_target_embeddings.shape[1])]\n",
    "source_weight_embeddings.columns = [\"Node\"] + [f\"Embed_{i}\" for i in range(1, source_weight_embeddings.shape[1])]\n",
    "weight_target_embeddings.columns = [\"Node\"] + [f\"Embed_{i}\" for i in range(1, weight_target_embeddings.shape[1])]\n",
    "\n",
    "# Merge embeddings with interaction data\n",
    "interaction_data = interaction_data.merge(source_target_embeddings, left_on=\"Source\", right_on=\"Node\", suffixes=(\"\", \"_Source\"))\n",
    "interaction_data = interaction_data.merge(source_target_embeddings, left_on=\"Target\", right_on=\"Node\", suffixes=(\"\", \"_Target\"))\n",
    "interaction_data = interaction_data.merge(book_labels, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Embed_2</th>\n",
       "      <th>Embed_3</th>\n",
       "      <th>Embed_4</th>\n",
       "      <th>Embed_5</th>\n",
       "      <th>Embed_6</th>\n",
       "      <th>Embed_7</th>\n",
       "      <th>Embed_8</th>\n",
       "      <th>...</th>\n",
       "      <th>Embed_57_Target</th>\n",
       "      <th>Embed_58_Target</th>\n",
       "      <th>Embed_59_Target</th>\n",
       "      <th>Embed_60_Target</th>\n",
       "      <th>Embed_61_Target</th>\n",
       "      <th>Embed_62_Target</th>\n",
       "      <th>Embed_63_Target</th>\n",
       "      <th>Embed_64_Target</th>\n",
       "      <th>Embed_65_Target</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Velerad</td>\n",
       "      <td>Geralt</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.046306</td>\n",
       "      <td>-0.032459</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>0.267487</td>\n",
       "      <td>-0.150255</td>\n",
       "      <td>0.138698</td>\n",
       "      <td>0.174296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243139</td>\n",
       "      <td>0.057464</td>\n",
       "      <td>-0.033725</td>\n",
       "      <td>-0.093554</td>\n",
       "      <td>0.249473</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.116918</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Velerad</td>\n",
       "      <td>Geralt</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.046306</td>\n",
       "      <td>-0.032459</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>0.267487</td>\n",
       "      <td>-0.150255</td>\n",
       "      <td>0.138698</td>\n",
       "      <td>0.174296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>0.033041</td>\n",
       "      <td>-0.052114</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.039098</td>\n",
       "      <td>-0.142371</td>\n",
       "      <td>0.108573</td>\n",
       "      <td>0.091120</td>\n",
       "      <td>-0.171687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Velerad</td>\n",
       "      <td>Geralt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062838</td>\n",
       "      <td>-0.026746</td>\n",
       "      <td>-0.017451</td>\n",
       "      <td>-0.109576</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>-0.071201</td>\n",
       "      <td>-0.094078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243139</td>\n",
       "      <td>0.057464</td>\n",
       "      <td>-0.033725</td>\n",
       "      <td>-0.093554</td>\n",
       "      <td>0.249473</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.116918</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Velerad</td>\n",
       "      <td>Geralt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062838</td>\n",
       "      <td>-0.026746</td>\n",
       "      <td>-0.017451</td>\n",
       "      <td>-0.109576</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>-0.071201</td>\n",
       "      <td>-0.094078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>0.033041</td>\n",
       "      <td>-0.052114</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.039098</td>\n",
       "      <td>-0.142371</td>\n",
       "      <td>0.108573</td>\n",
       "      <td>0.091120</td>\n",
       "      <td>-0.171687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foltest</td>\n",
       "      <td>Geralt</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055646</td>\n",
       "      <td>-0.038783</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.111643</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>-0.077998</td>\n",
       "      <td>-0.068830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243139</td>\n",
       "      <td>0.057464</td>\n",
       "      <td>-0.033725</td>\n",
       "      <td>-0.093554</td>\n",
       "      <td>0.249473</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.116918</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>Henselt</td>\n",
       "      <td>King</td>\n",
       "      <td>2</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>-0.033965</td>\n",
       "      <td>-0.033946</td>\n",
       "      <td>-0.123002</td>\n",
       "      <td>-0.007751</td>\n",
       "      <td>-0.081750</td>\n",
       "      <td>-0.071421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241330</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>-0.038299</td>\n",
       "      <td>-0.091706</td>\n",
       "      <td>0.258406</td>\n",
       "      <td>-0.004897</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.108766</td>\n",
       "      <td>-0.013695</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>Henselt</td>\n",
       "      <td>King</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.073236</td>\n",
       "      <td>-0.034930</td>\n",
       "      <td>-0.181299</td>\n",
       "      <td>0.264487</td>\n",
       "      <td>-0.138133</td>\n",
       "      <td>0.121084</td>\n",
       "      <td>0.158849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024614</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>-0.066351</td>\n",
       "      <td>-0.267194</td>\n",
       "      <td>0.055483</td>\n",
       "      <td>-0.131152</td>\n",
       "      <td>0.109692</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>-0.174639</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>Henselt</td>\n",
       "      <td>King</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.073236</td>\n",
       "      <td>-0.034930</td>\n",
       "      <td>-0.181299</td>\n",
       "      <td>0.264487</td>\n",
       "      <td>-0.138133</td>\n",
       "      <td>0.121084</td>\n",
       "      <td>0.158849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241330</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>-0.038299</td>\n",
       "      <td>-0.091706</td>\n",
       "      <td>0.258406</td>\n",
       "      <td>-0.004897</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.108766</td>\n",
       "      <td>-0.013695</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>Philippa</td>\n",
       "      <td>King</td>\n",
       "      <td>3</td>\n",
       "      <td>0.070165</td>\n",
       "      <td>-0.033380</td>\n",
       "      <td>-0.041951</td>\n",
       "      <td>-0.121565</td>\n",
       "      <td>-0.013871</td>\n",
       "      <td>-0.081241</td>\n",
       "      <td>-0.062850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024614</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>-0.066351</td>\n",
       "      <td>-0.267194</td>\n",
       "      <td>0.055483</td>\n",
       "      <td>-0.131152</td>\n",
       "      <td>0.109692</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>-0.174639</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>Philippa</td>\n",
       "      <td>King</td>\n",
       "      <td>3</td>\n",
       "      <td>0.070165</td>\n",
       "      <td>-0.033380</td>\n",
       "      <td>-0.041951</td>\n",
       "      <td>-0.121565</td>\n",
       "      <td>-0.013871</td>\n",
       "      <td>-0.081241</td>\n",
       "      <td>-0.062850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241330</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>-0.038299</td>\n",
       "      <td>-0.091706</td>\n",
       "      <td>0.258406</td>\n",
       "      <td>-0.004897</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.108766</td>\n",
       "      <td>-0.013695</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2600 rows Ã— 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source  Target  Weight   Embed_2   Embed_3   Embed_4   Embed_5  \\\n",
       "0      Velerad  Geralt       1 -0.046306 -0.032459 -0.164166  0.267487   \n",
       "1      Velerad  Geralt       1 -0.046306 -0.032459 -0.164166  0.267487   \n",
       "2      Velerad  Geralt       1  0.062838 -0.026746 -0.017451 -0.109576   \n",
       "3      Velerad  Geralt       1  0.062838 -0.026746 -0.017451 -0.109576   \n",
       "4      Foltest  Geralt       4  0.055646 -0.038783 -0.032035 -0.111643   \n",
       "...        ...     ...     ...       ...       ...       ...       ...   \n",
       "2595   Henselt    King       2  0.065173 -0.033965 -0.033946 -0.123002   \n",
       "2596   Henselt    King       2 -0.073236 -0.034930 -0.181299  0.264487   \n",
       "2597   Henselt    King       2 -0.073236 -0.034930 -0.181299  0.264487   \n",
       "2598  Philippa    King       3  0.070165 -0.033380 -0.041951 -0.121565   \n",
       "2599  Philippa    King       3  0.070165 -0.033380 -0.041951 -0.121565   \n",
       "\n",
       "       Embed_6   Embed_7   Embed_8  ...  Embed_57_Target  Embed_58_Target  \\\n",
       "0    -0.150255  0.138698  0.174296  ...        -0.243139         0.057464   \n",
       "1    -0.150255  0.138698  0.174296  ...         0.022619         0.033041   \n",
       "2     0.028481 -0.071201 -0.094078  ...        -0.243139         0.057464   \n",
       "3     0.028481 -0.071201 -0.094078  ...         0.022619         0.033041   \n",
       "4    -0.007959 -0.077998 -0.068830  ...        -0.243139         0.057464   \n",
       "...        ...       ...       ...  ...              ...              ...   \n",
       "2595 -0.007751 -0.081750 -0.071421  ...        -0.241330         0.059972   \n",
       "2596 -0.138133  0.121084  0.158849  ...         0.024614         0.027329   \n",
       "2597 -0.138133  0.121084  0.158849  ...        -0.241330         0.059972   \n",
       "2598 -0.013871 -0.081241 -0.062850  ...         0.024614         0.027329   \n",
       "2599 -0.013871 -0.081241 -0.062850  ...        -0.241330         0.059972   \n",
       "\n",
       "      Embed_59_Target  Embed_60_Target  Embed_61_Target  Embed_62_Target  \\\n",
       "0           -0.033725        -0.093554         0.249473         0.007179   \n",
       "1           -0.052114        -0.278882         0.039098        -0.142371   \n",
       "2           -0.033725        -0.093554         0.249473         0.007179   \n",
       "3           -0.052114        -0.278882         0.039098        -0.142371   \n",
       "4           -0.033725        -0.093554         0.249473         0.007179   \n",
       "...               ...              ...              ...              ...   \n",
       "2595        -0.038299        -0.091706         0.258406        -0.004897   \n",
       "2596        -0.066351        -0.267194         0.055483        -0.131152   \n",
       "2597        -0.038299        -0.091706         0.258406        -0.004897   \n",
       "2598        -0.066351        -0.267194         0.055483        -0.131152   \n",
       "2599        -0.038299        -0.091706         0.258406        -0.004897   \n",
       "\n",
       "      Embed_63_Target  Embed_64_Target  Embed_65_Target  book  \n",
       "0            0.017604         0.116918        -0.008722     1  \n",
       "1            0.108573         0.091120        -0.171687     1  \n",
       "2            0.017604         0.116918        -0.008722     1  \n",
       "3            0.108573         0.091120        -0.171687     1  \n",
       "4            0.017604         0.116918        -0.008722     1  \n",
       "...               ...              ...              ...   ...  \n",
       "2595         0.010340         0.108766        -0.013695     7  \n",
       "2596         0.109692         0.083849        -0.174639     7  \n",
       "2597         0.010340         0.108766        -0.013695     7  \n",
       "2598         0.109692         0.083849        -0.174639     7  \n",
       "2599         0.010340         0.108766        -0.013695     7  \n",
       "\n",
       "[2600 rows x 134 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_data.drop(columns=[\"Node\", \"Embed_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "# Combine source and target embeddings (e.g., concatenation)\n",
    "source_emb_cols = [col for col in interaction_data.columns if \"Embed_\" in col and \"_Source\" in col]\n",
    "target_emb_cols = [col for col in interaction_data.columns if \"Embed_\" in col and \"_Target\" in col]\n",
    "\n",
    "interaction_data[\"Weight_Normalized\"] = interaction_data[\"Weight\"] / interaction_data[\"Weight\"].max()\n",
    "\n",
    "# Example: Concatenate source and target embeddings\n",
    "interaction_data[\"Features\"] = interaction_data[source_emb_cols + target_emb_cols].values.tolist()\n",
    "\n",
    "# Step 3: Train/Test Split\n",
    "X = np.stack(interaction_data[\"Features\"])\n",
    "y = interaction_data[\"book\"]  # Target label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 65)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "# Step 5: Train Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.732935088938899\n",
      "Accuracy: 0.7730769230769231\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        53\n",
      "           2       0.00      0.00      0.00        32\n",
      "           3       0.38      1.00      0.55        60\n",
      "           4       0.95      0.82      0.88        85\n",
      "           5       1.00      0.94      0.97        90\n",
      "           6       0.98      0.81      0.89        68\n",
      "           7       0.91      1.00      0.95       132\n",
      "\n",
      "    accuracy                           0.77       520\n",
      "   macro avg       0.60      0.65      0.61       520\n",
      "weighted avg       0.73      0.77      0.73       520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# print per book classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
